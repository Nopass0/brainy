/**
 * –ü—Ä–∏–º–µ—Ä 06: –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Å–ª–æ—ë–≤
 * 
 * –≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç:
 * - –ù–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç Module –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–æ–∏—Ö —Å–ª–æ—ë–≤
 * - –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø–æ–¥–º–æ–¥—É–ª–µ–π
 * - –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä (Residual connections)
 * - Attention –º–µ—Ö–∞–Ω–∏–∑–º
 * 
 * –ó–∞–ø—É—Å–∫: bun run examples/06-custom-layer.ts
 */

import {
  tensor,
  randn,
  zeros,
  ones,
  Module,
  Parameter,
  Linear,
  ReLU,
  Sigmoid,
  Tensor,
  Sequential,
  MSELoss,
  Adam,
  softmax,
  Dropout,
  LayerNorm,
} from '../src';

console.log('üß† Brainy - –ü—Ä–∏–º–µ—Ä 06: –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Å–ª–æ–∏\n');
console.log('='.repeat(60));

// ============================================
// 1. –ü—Ä–æ—Å—Ç–æ–π –∫–∞—Å—Ç–æ–º–Ω—ã–π —Å–ª–æ–π: Swish –∞–∫—Ç–∏–≤–∞—Ü–∏—è
// ============================================
console.log('\nüîß 1. –ö–∞—Å—Ç–æ–º–Ω—ã–π —Å–ª–æ–π: Swish (x * sigmoid(x))\n');

/**
 * Swish –∞–∫—Ç–∏–≤–∞—Ü–∏—è: f(x) = x * sigmoid(x)
 * –ü–æ–ø—É–ª—è—Ä–Ω–∞—è –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö
 */
class Swish extends Module {
  forward(x: Tensor): Tensor {
    const sigmoid_x = x.exp().div(x.exp().add(1));
    return x.mul(sigmoid_x);
  }

  toString(): string {
    return 'Swish()';
  }
}

const swish = new Swish();
const testInput = tensor([-2, -1, 0, 1, 2]);
const swishOutput = swish.forward(testInput);

console.log(`–í—Ö–æ–¥: ${testInput.toArray()}`);
console.log(`Swish –≤—ã—Ö–æ–¥: ${swishOutput.toArray().map((x: number) => x.toFixed(4))}`);
console.log('‚úÖ Swish —Å–ª–æ–π —Ä–∞–±–æ—Ç–∞–µ—Ç!');

// ============================================
// 2. –°–ª–æ–π —Å –æ–±—É—á–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: ScaleShift
// ============================================
console.log('\n' + '='.repeat(60));
console.log('\nüîß 2. –°–ª–æ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: ScaleShift\n');

/**
 * ScaleShift: y = scale * x + shift
 * –ü—Ä–æ—Å—Ç–æ–π —Å–ª–æ–π —Å –¥–≤—É–º—è –æ–±—É—á–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
 */
class ScaleShift extends Module {
  scale: Parameter;
  shift: Parameter;
  readonly features: number;

  constructor(features: number) {
    super();
    this.features = features;
    
    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: scale=1, shift=0
    this.scale = new Parameter(ones([features], { requiresGrad: true }), 'scale');
    this.shift = new Parameter(zeros([features], { requiresGrad: true }), 'shift');
    
    this.registerParameter('scale', this.scale);
    this.registerParameter('shift', this.shift);
  }

  forward(x: Tensor): Tensor {
    return x.mul(this.scale.data).add(this.shift.data);
  }

  toString(): string {
    return `ScaleShift(features=${this.features})`;
  }
}

const scaleShift = new ScaleShift(4);
console.log(`ScaleShift: ${scaleShift.toString()}`);
console.log(`–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: ${scaleShift.numParameters()}`);

const ssInput = tensor([[1, 2, 3, 4]]);
console.log(`–í—Ö–æ–¥: ${ssInput.toArray()}`);
console.log(`–í—ã—Ö–æ–¥ (scale=1, shift=0): ${scaleShift.forward(ssInput).toArray()}`);

// –ò–∑–º–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
for (let i = 0; i < scaleShift.scale.data.size; i++) {
  (scaleShift.scale.data.data as Float32Array)[i] = 2;
  (scaleShift.shift.data.data as Float32Array)[i] = 10;
}
console.log(`–í—ã—Ö–æ–¥ (scale=2, shift=10): ${scaleShift.forward(ssInput).toArray()}`);
console.log('‚úÖ ScaleShift —Å–ª–æ–π —Ä–∞–±–æ—Ç–∞–µ—Ç!');

// ============================================
// 3. Residual Block
// ============================================
console.log('\n' + '='.repeat(60));
console.log('\nüîß 3. Residual Block (ResNet-style)\n');

/**
 * Residual Block: output = F(x) + x
 * –ö–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç ResNet –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
 */
class ResidualBlock extends Module {
  linear1: Linear;
  linear2: Linear;
  relu: ReLU;
  readonly features: number;

  constructor(features: number) {
    super();
    this.features = features;
    
    this.linear1 = new Linear(features, features);
    this.linear2 = new Linear(features, features);
    this.relu = new ReLU();
    
    this.registerModule('linear1', this.linear1);
    this.registerModule('linear2', this.linear2);
  }

  forward(x: Tensor): Tensor {
    // F(x) = linear2(relu(linear1(x)))
    let residual = this.linear1.forward(x);
    residual = this.relu.forward(residual);
    residual = this.linear2.forward(x);
    
    // output = F(x) + x (skip connection)
    return residual.add(x);
  }

  toString(): string {
    return `ResidualBlock(\n  (linear1): ${this.linear1}\n  (relu): ReLU()\n  (linear2): ${this.linear2}\n)`;
  }
}

const resBlock = new ResidualBlock(8);
console.log('–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ ResidualBlock:');
console.log('‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê');
console.log('‚îÇ        Input (x)            ‚îÇ');
console.log('‚îÇ           ‚Üì                 ‚îÇ');
console.log('‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ');
console.log('‚îÇ    ‚Üì             ‚Üì          ‚îÇ');
console.log('‚îÇ  Linear1      Identity      ‚îÇ');
console.log('‚îÇ    ‚Üì             ‚îÇ          ‚îÇ');
console.log('‚îÇ   ReLU          ‚îÇ          ‚îÇ');
console.log('‚îÇ    ‚Üì             ‚îÇ          ‚îÇ');
console.log('‚îÇ  Linear2        ‚îÇ          ‚îÇ');
console.log('‚îÇ    ‚Üì             ‚îÇ          ‚îÇ');
console.log('‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ+‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ');
console.log('‚îÇ           ‚Üì                 ‚îÇ');
console.log('‚îÇ       Output                ‚îÇ');
console.log('‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò');
console.log(`–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: ${resBlock.numParameters()}`);

const resInput = randn([2, 8]);
const resOutput = resBlock.forward(resInput);
console.log(`\n–í—Ö–æ–¥ shape: [${resInput.shape}]`);
console.log(`–í—ã—Ö–æ–¥ shape: [${resOutput.shape}]`);
console.log('‚úÖ ResidualBlock —Ä–∞–±–æ—Ç–∞–µ—Ç!');

// ============================================
// 4. Self-Attention Layer (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è)
// ============================================
console.log('\n' + '='.repeat(60));
console.log('\nüîß 4. Self-Attention (Transformer-style)\n');

/**
 * –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π Self-Attention –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–∞
 * Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V
 */
class SelfAttention extends Module {
  queryProj: Linear;
  keyProj: Linear;
  valueProj: Linear;
  outProj: Linear;
  readonly embedDim: number;
  readonly scale: number;

  constructor(embedDim: number) {
    super();
    this.embedDim = embedDim;
    this.scale = Math.sqrt(embedDim);
    
    this.queryProj = new Linear(embedDim, embedDim, false);
    this.keyProj = new Linear(embedDim, embedDim, false);
    this.valueProj = new Linear(embedDim, embedDim, false);
    this.outProj = new Linear(embedDim, embedDim, false);
    
    this.registerModule('query', this.queryProj);
    this.registerModule('key', this.keyProj);
    this.registerModule('value', this.valueProj);
    this.registerModule('out', this.outProj);
  }

  forward(x: Tensor): Tensor {
    // x: [batch, seq_len, embed_dim] –∏–ª–∏ [seq_len, embed_dim]
    
    // –ü—Ä–æ–µ–∫—Ü–∏–∏ Q, K, V
    const Q = this.queryProj.forward(x);  // [batch, seq, embed]
    const K = this.keyProj.forward(x);
    const V = this.valueProj.forward(x);
    
    // Attention scores: QK^T / sqrt(d)
    // –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Ä–∞–±–æ—Ç–∞–µ–º —Å 2D: [seq, embed]
    const scores = Q.matmul(K.T).div(this.scale);  // [seq, seq]
    
    // Softmax
    const weights = softmax(scores, -1);  // [seq, seq]
    
    // Weighted values
    const attended = weights.matmul(V);  // [seq, embed]
    
    // Output projection
    return this.outProj.forward(attended);
  }

  toString(): string {
    return `SelfAttention(embed_dim=${this.embedDim})`;
  }
}

const attention = new SelfAttention(16);
console.log('Self-Attention:');
console.log('  Q = W_q * X');
console.log('  K = W_k * X');
console.log('  V = W_v * X');
console.log('  Attention = softmax(QK^T / ‚àöd) * V');
console.log(`\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: ${attention.numParameters()}`);

const seqLen = 5;
const attInput = randn([seqLen, 16]);  // [seq, embed]
const attOutput = attention.forward(attInput);
console.log(`\n–í—Ö–æ–¥ shape: [${attInput.shape}] (seq_len=5, embed=16)`);
console.log(`–í—ã—Ö–æ–¥ shape: [${attOutput.shape}]`);
console.log('‚úÖ Self-Attention —Ä–∞–±–æ—Ç–∞–µ—Ç!');

// ============================================
// 5. –°–±–æ—Ä–∫–∞ –≤ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
// ============================================
console.log('\n' + '='.repeat(60));
console.log('\nüèóÔ∏è 5. –°–±–æ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n');

/**
 * –ü—Ä–æ—Å—Ç–æ–π Transformer-style —ç–Ω–∫–æ–¥–µ—Ä
 */
class MiniTransformerBlock extends Module {
  attention: SelfAttention;
  ffn: Sequential;
  norm1: LayerNorm;
  norm2: LayerNorm;

  constructor(embedDim: number, ffnDim: number) {
    super();
    
    this.attention = new SelfAttention(embedDim);
    this.ffn = new Sequential(
      new Linear(embedDim, ffnDim),
      new ReLU(),
      new Linear(ffnDim, embedDim)
    );
    this.norm1 = new LayerNorm(embedDim);
    this.norm2 = new LayerNorm(embedDim);
    
    this.registerModule('attention', this.attention);
    this.registerModule('ffn', this.ffn);
    this.registerModule('norm1', this.norm1);
    this.registerModule('norm2', this.norm2);
  }

  forward(x: Tensor): Tensor {
    // Pre-norm architecture
    // x = x + Attention(Norm(x))
    const normed1 = this.norm1.forward(x);
    const attended = this.attention.forward(normed1);
    let out = x.add(attended);
    
    // x = x + FFN(Norm(x))
    const normed2 = this.norm2.forward(out);
    const ffnOut = this.ffn.forward(normed2);
    out = out.add(ffnOut);
    
    return out;
  }

  toString(): string {
    return 'MiniTransformerBlock(\n  attention + residual\n  ffn + residual\n)';
  }
}

const transformer = new MiniTransformerBlock(16, 64);
console.log('Mini Transformer Block:');
console.log('‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê');
console.log('‚îÇ           Input                     ‚îÇ');
console.log('‚îÇ             ‚Üì                       ‚îÇ');
console.log('‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ');
console.log('‚îÇ    ‚Üì               ‚Üì                ‚îÇ');
console.log('‚îÇ  LayerNorm      Identity            ‚îÇ');
console.log('‚îÇ    ‚Üì               ‚îÇ                ‚îÇ');
console.log('‚îÇ  Self-Attn        ‚îÇ                ‚îÇ');
console.log('‚îÇ    ‚Üì               ‚îÇ                ‚îÇ');
console.log('‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ+‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ');
console.log('‚îÇ             ‚Üì                       ‚îÇ');
console.log('‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ');
console.log('‚îÇ    ‚Üì               ‚Üì                ‚îÇ');
console.log('‚îÇ  LayerNorm      Identity            ‚îÇ');
console.log('‚îÇ    ‚Üì               ‚îÇ                ‚îÇ');
console.log('‚îÇ    FFN            ‚îÇ                ‚îÇ');
console.log('‚îÇ    ‚Üì               ‚îÇ                ‚îÇ');
console.log('‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ+‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ');
console.log('‚îÇ             ‚Üì                       ‚îÇ');
console.log('‚îÇ          Output                     ‚îÇ');
console.log('‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò');
console.log(`\n–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: ${transformer.numParameters()}`);

const tfInput = randn([4, 16]);  // [seq=4, embed=16]
const tfOutput = transformer.forward(tfInput);
console.log(`\n–í—Ö–æ–¥ shape: [${tfInput.shape}]`);
console.log(`–í—ã—Ö–æ–¥ shape: [${tfOutput.shape}]`);
console.log('‚úÖ Mini Transformer Block —Ä–∞–±–æ—Ç–∞–µ—Ç!');

// ============================================
// 6. –û–±—É—á–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏
// ============================================
console.log('\n' + '='.repeat(60));
console.log('\nüéì 6. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ —Å–ª–æ—è–º–∏\n');

// –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ —Å–ª–æ—è–º–∏
class CustomModel extends Module {
  block1: ResidualBlock;
  block2: ResidualBlock;
  output: Linear;

  constructor(features: number, outputDim: number) {
    super();
    this.block1 = new ResidualBlock(features);
    this.block2 = new ResidualBlock(features);
    this.output = new Linear(features, outputDim);
    
    this.registerModule('block1', this.block1);
    this.registerModule('block2', this.block2);
    this.registerModule('output', this.output);
  }

  forward(x: Tensor): Tensor {
    let out = this.block1.forward(x);
    out = this.block2.forward(out);
    return this.output.forward(out);
  }
}

const model = new CustomModel(8, 2);
const criterion = new MSELoss();
const optimizer = new Adam(model.parameters(), 0.01);

console.log(`–ú–æ–¥–µ–ª—å —Å ${model.numParameters()} –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏`);
console.log('–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\n');

const trainX = randn([16, 8]);
const trainY = randn([16, 2]);

for (let epoch = 0; epoch < 50; epoch++) {
  const pred = model.forward(trainX);
  const loss = criterion.forward(pred, trainY);
  
  optimizer.zeroGrad();
  loss.backward();
  optimizer.step();
  
  if (epoch % 10 === 0 || epoch === 49) {
    console.log(`–≠–ø–æ—Ö–∞ ${epoch}: loss = ${loss.item().toFixed(4)}`);
  }
}

console.log('\n‚úÖ –ú–æ–¥–µ–ª—å —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ —Å–ª–æ—è–º–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è!');

// ============================================
// –ò—Ç–æ–≥–∏
// ============================================
console.log('\n' + '='.repeat(60));
console.log('\nüéâ –í—Å–µ –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Å–ª–æ–∏ —Ä–∞–±–æ—Ç–∞—é—Ç!');
console.log('\nüìù –†–µ–∑—é–º–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Å–ª–æ—ë–≤:');
console.log('  1. Swish - –ø—Ä–æ—Å—Ç–∞—è –∫–∞—Å—Ç–æ–º–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è');
console.log('  2. ScaleShift - —Å–ª–æ–π —Å –æ–±—É—á–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏');
console.log('  3. ResidualBlock - skip connections (ResNet)');
console.log('  4. SelfAttention - –º–µ—Ö–∞–Ω–∏–∑–º attention (Transformer)');
console.log('  5. MiniTransformerBlock - –ø–æ–ª–Ω—ã–π –±–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞');
console.log('\n‚úÖ Brainy –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –ª—é–±—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã!');
